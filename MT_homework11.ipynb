{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperCoolCucumber/CompLing/blob/main/MT_homework11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.uk\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-test.uk\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VayheSm4DT",
        "outputId": "de2c13fc-cf9b-4572-e7fb-eafdf5fb60df"
      },
      "id": "o1VayheSm4DT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-24 13:50:45--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.uk\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62317727 (59M)\n",
            "Saving to: ‘opus.en-uk-train.uk’\n",
            "\n",
            "opus.en-uk-train.uk 100%[===================>]  59.43M  20.9MB/s    in 2.8s    \n",
            "\n",
            "2022-05-24 13:50:49 (20.9 MB/s) - ‘opus.en-uk-train.uk’ saved [62317727/62317727]\n",
            "\n",
            "--2022-05-24 13:50:49--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35911829 (34M)\n",
            "Saving to: ‘opus.en-uk-train.en’\n",
            "\n",
            "opus.en-uk-train.en 100%[===================>]  34.25M  18.7MB/s    in 1.8s    \n",
            "\n",
            "2022-05-24 13:50:51 (18.7 MB/s) - ‘opus.en-uk-train.en’ saved [35911829/35911829]\n",
            "\n",
            "--2022-05-24 13:50:51--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-test.uk\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149454 (146K)\n",
            "Saving to: ‘opus.en-uk-test.uk’\n",
            "\n",
            "opus.en-uk-test.uk  100%[===================>] 145.95K   491KB/s    in 0.3s    \n",
            "\n",
            "2022-05-24 13:50:52 (491 KB/s) - ‘opus.en-uk-test.uk’ saved [149454/149454]\n",
            "\n",
            "--2022-05-24 13:50:52--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88288 (86K)\n",
            "Saving to: ‘opus.en-uk-test.en’\n",
            "\n",
            "opus.en-uk-test.en  100%[===================>]  86.22K   435KB/s    in 0.2s    \n",
            "\n",
            "2022-05-24 13:50:53 (435 KB/s) - ‘opus.en-uk-test.en’ saved [88288/88288]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "slys9fOLH3ZB"
      },
      "id": "slys9fOLH3ZB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('opus.en-uk-train.uk').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.uk', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "VcfDlreT8FeX"
      },
      "id": "VcfDlreT8FeX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-uk-train.en').read().splitlines()\n",
        "uk_sents = open('opus.en-uk-train.uk').read().splitlines()"
      ],
      "metadata": {
        "id": "yGSJbQHJI5Py"
      },
      "id": "yGSJbQHJI5Py",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mXRNa2w9LX",
        "outputId": "c8777a71-e421-478c-806e-fdc7d50ed97a"
      },
      "id": "a8mXRNa2w9LX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(uk_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKaMYCE0xFye",
        "outputId": "fbac125c-bc85-47a4-ce48-4845246656a5"
      },
      "id": "DKaMYCE0xFye",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NW86FVH5xSRX",
        "outputId": "75b1bb6a-05e0-405f-f38f-4e80a7c9ed99"
      },
      "id": "NW86FVH5xSRX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yeah. It really sucks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uk_sents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qjZ0WD0BxVKd",
        "outputId": "dff93299-511b-4ef8-acb5-630bada207fc"
      },
      "id": "qjZ0WD0BxVKd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Да.Это хреново.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-uk-train.en\"], trainer=trainer_en)\n",
        "\n",
        "tokenizer_uk = Tokenizer(BPE())\n",
        "tokenizer_uk.pre_tokenizer = Whitespace()\n",
        "trainer_uk = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_uk.train(files=[\"opus.en-uk-train.uk\"], trainer=trainer_uk)"
      ],
      "metadata": {
        "id": "XhN80Hs3YbDI"
      },
      "id": "XhN80Hs3YbDI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_uk.save('tokenizer_uk')"
      ],
      "metadata": {
        "id": "9jFKG82sbyC0"
      },
      "id": "9jFKG82sbyC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_uk = Tokenizer.from_file(\"tokenizer_uk\")"
      ],
      "metadata": {
        "id": "UAjPeDAdbzAN"
      },
      "id": "UAjPeDAdbzAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
      ],
      "metadata": {
        "id": "QInFsmdIb-BZ"
      },
      "id": "QInFsmdIb-BZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_uk.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "HA-nWnmVcgFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92a08c8-7743-4b57-8b88-6a399a8caced"
      },
      "id": "HA-nWnmVcgFG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_uk = 50, 50"
      ],
      "metadata": {
        "id": "uLx_1enTclo1"
      },
      "id": "uLx_1enTclo1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_uk = [encode(t, tokenizer_uk, max_len_uk) for t in uk_sents]"
      ],
      "metadata": {
        "id": "c6mptChrcoyS"
      },
      "id": "c6mptChrcoyS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao5p7BOU2zzy",
        "outputId": "f4dc31a6-0a53-470f-aa62-2c2f35e7777d"
      },
      "id": "ao5p7BOU2zzy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_uk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZQur69s3and",
        "outputId": "2ddddcaa-9336-4de6-9e96-6dfa988ccea2"
      },
      "id": "yZQur69s3and",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_uk):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
        "        \n",
        "        self.texts_uk = [torch.LongTensor(sent) for sent in texts_uk]\n",
        "        self.texts_uk = torch.nn.utils.rnn.pad_sequence(self.texts_uk, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_en)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[:, index]\n",
        "        ids_uk = self.texts_uk[:, index]\n",
        "\n",
        "        return ids_en, ids_uk"
      ],
      "metadata": {
        "id": "_Dqz1XnYc7Hz"
      },
      "id": "_Dqz1XnYc7Hz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en_train, X_en_valid, X_uk_train, X_uk_valid = train_test_split(X_en, X_uk, test_size=0.05)"
      ],
      "metadata": {
        "id": "A1Jz2z0UdRJp"
      },
      "id": "A1Jz2z0UdRJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(X_en_train, X_uk_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )"
      ],
      "metadata": {
        "id": "Pol-Gw6GdUoJ"
      },
      "id": "Pol-Gw6GdUoJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_set = Dataset(X_en_valid, X_uk_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
      ],
      "metadata": {
        "id": "tDbG5jplddFD"
      },
      "id": "tDbG5jplddFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e15bQdQIn3H8"
      },
      "id": "e15bQdQIn3H8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer (from [pytorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html))"
      ],
      "metadata": {
        "id": "KNMHN3-9ADf-"
      },
      "id": "KNMHN3-9ADf-"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "#         print('pos inp')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "#         print('pos dec')\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "#         print('pos out')\n",
        "        x = self.generator(outs)\n",
        "#         print('gen')\n",
        "        return x\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    \n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "Ghf-rAuNAC3m"
      },
      "id": "Ghf-rAuNAC3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, print_every=500):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "    \n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts_en, texts_uk) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) \n",
        "        texts_uk = texts_uk.T.to(DEVICE) \n",
        "        \n",
        "\n",
        "        texts_uk_input = texts_uk[:-1, :]\n",
        "        \n",
        "\n",
        "        (texts_en_mask, texts_uk_mask, \n",
        "        texts_en_padding_mask, texts_uk_padding_mask) = create_mask(texts_en, texts_uk_input)\n",
        "        logits = model(texts_en, texts_uk_input, texts_en_mask, texts_uk_mask,\n",
        "                       texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        texts_uk_out = texts_uk[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_uk_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        \n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, (texts_en, texts_uk) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) \n",
        "        texts_uk = texts_uk.T.to(DEVICE)\n",
        "        texts_uk_input = texts_uk[:-1, :]\n",
        "\n",
        "        \n",
        "        (texts_en_mask, texts_uk_mask, \n",
        "        texts_en_padding_mask, texts_uk_padding_mask) = create_mask(texts_en, texts_uk_input)\n",
        "        logits = model(texts_en, texts_uk_input, texts_en_mask, texts_uk_mask,\n",
        "                       texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
        "        \n",
        "        texts_uk_out = texts_uk[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_uk_out.reshape(-1))\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        \n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "zwcWWA9tAcWu"
      },
      "id": "zwcWWA9tAcWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "UK_VOCAB_SIZE = tokenizer_uk.get_vocab_size()\n",
        "\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, EN_VOCAB_SIZE, UK_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "agC_xwJWAcTy"
      },
      "id": "agC_xwJWAcTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(transformer, 'model')"
      ],
      "metadata": {
        "id": "VbHmOawgAcP8"
      },
      "id": "VbHmOawgAcP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer = torch.load('model').to(DEVICE)"
      ],
      "metadata": {
        "id": "wGk3JFKzAcLh"
      },
      "id": "wGk3JFKzAcLh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JBhn_RLSAcGx"
      },
      "id": "JBhn_RLSAcGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
        "    \n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    losses.append(val_loss)\n",
        "        \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyc-IcFfAcDr",
        "outputId": "29729d26-86a6-4aa0-e268-74da7469c872"
      },
      "id": "iyc-IcFfAcDr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.4664886770248415;\n",
            "Loss: 6.881732636451721;\n",
            "Loss: 6.580780406951904;\n",
            "Loss: 6.369524511575698;\n",
            "Loss: 6.2078326839447024;\n",
            "Loss: 6.077315409819285;\n",
            "Loss: 5.9663321219853;\n",
            "Loss: 5.8682837243080135;\n",
            "Loss: 5.7823254144456655;\n",
            "First epoch - 4.874063325881958, saving model..\n",
            "Epoch: 1, Train loss: 5.742, Val loss: 4.874,            Epoch time=1227.709s\n",
            "Loss: 4.91703074836731;\n",
            "Loss: 4.880656561374664;\n",
            "Loss: 4.846615245501201;\n",
            "Loss: 4.813653960466385;\n",
            "Loss: 4.780827764892578;\n",
            "Loss: 4.748732059955597;\n",
            "Loss: 4.717852898733956;\n",
            "Loss: 4.689744362950325;\n",
            "Loss: 4.6613774173524645;\n",
            "Improved from 4.874063325881958 to 4.2287362213134765, saving model..\n",
            "Epoch: 2, Train loss: 4.647, Val loss: 4.229,            Epoch time=1234.150s\n",
            "Loss: 4.294831292629242;\n",
            "Loss: 4.279468092918396;\n",
            "Loss: 4.259419092019399;\n",
            "Loss: 4.238797508478164;\n",
            "Loss: 4.219801478672028;\n",
            "Loss: 4.200634664058685;\n",
            "Loss: 4.181723873751504;\n",
            "Loss: 4.164667080402374;\n",
            "Loss: 4.147414542145199;\n",
            "Improved from 4.2287362213134765 to 3.818355595588684, saving model..\n",
            "Epoch: 3, Train loss: 4.138, Val loss: 3.818,            Epoch time=1230.555s\n",
            "Loss: 3.8757149782180784;\n",
            "Loss: 3.8676730613708497;\n",
            "Loss: 3.859632898489634;\n",
            "Loss: 3.8503082814216616;\n",
            "Loss: 3.8382123759269713;\n",
            "Loss: 3.827964469353358;\n",
            "Loss: 3.816615628991808;\n",
            "Loss: 3.8065255565047265;\n",
            "Loss: 3.7951412154833477;\n",
            "Improved from 3.818355595588684 to 3.5404057750701905, saving model..\n",
            "Epoch: 4, Train loss: 3.789, Val loss: 3.540,            Epoch time=1224.224s\n",
            "Loss: 3.580406826019287;\n",
            "Loss: 3.57678249502182;\n",
            "Loss: 3.5737567798296612;\n",
            "Loss: 3.5683693149089812;\n",
            "Loss: 3.562949294757843;\n",
            "Loss: 3.5569904441038767;\n",
            "Loss: 3.550114522048405;\n",
            "Loss: 3.545167248725891;\n",
            "Loss: 3.54000882768631;\n",
            "Improved from 3.5404057750701905 to 3.3410997772216797, saving model..\n",
            "Epoch: 5, Train loss: 3.537, Val loss: 3.341,            Epoch time=1223.829s\n",
            "Loss: 3.376679070472717;\n",
            "Loss: 3.3712113411426543;\n",
            "Loss: 3.3692049377759297;\n",
            "Loss: 3.368057419538498;\n",
            "Loss: 3.362633359622955;\n",
            "Loss: 3.3597738829453787;\n",
            "Loss: 3.357887688704899;\n",
            "Loss: 3.353879560530186;\n",
            "Loss: 3.349879206445482;\n",
            "Improved from 3.3410997772216797 to 3.2052221450805662, saving model..\n",
            "Epoch: 6, Train loss: 3.349, Val loss: 3.205,            Epoch time=1225.800s\n",
            "Loss: 3.2087968754768372;\n",
            "Loss: 3.212444787979126;\n",
            "Loss: 3.2105805883407594;\n",
            "Loss: 3.214319545984268;\n",
            "Loss: 3.2127083567619326;\n",
            "Loss: 3.2113886737823485;\n",
            "Loss: 3.2094053523881096;\n",
            "Loss: 3.2079124913215638;\n",
            "Loss: 3.2067733636432223;\n",
            "Improved from 3.2052221450805662 to 3.1042531452178954, saving model..\n",
            "Epoch: 7, Train loss: 3.206, Val loss: 3.104,            Epoch time=1223.448s\n",
            "Loss: 3.080856381416321;\n",
            "Loss: 3.0867212669849398;\n",
            "Loss: 3.091865301926931;\n",
            "Loss: 3.092843006849289;\n",
            "Loss: 3.093496223258972;\n",
            "Loss: 3.0948778268496198;\n",
            "Loss: 3.0938957864897594;\n",
            "Loss: 3.0955790978074074;\n",
            "Loss: 3.0949850102000767;\n",
            "Improved from 3.1042531452178954 to 3.026723942756653, saving model..\n",
            "Epoch: 8, Train loss: 3.095, Val loss: 3.027,            Epoch time=1231.045s\n",
            "Loss: 2.9889356217384337;\n",
            "Loss: 2.988291648864746;\n",
            "Loss: 2.98826970688502;\n",
            "Loss: 2.991272205352783;\n",
            "Loss: 2.9954757975578308;\n",
            "Loss: 2.9986672869523368;\n",
            "Loss: 3.0009541164125717;\n",
            "Loss: 3.003451959013939;\n",
            "Loss: 3.004430412504408;\n",
            "Improved from 3.026723942756653 to 2.9759431314468383, saving model..\n",
            "Epoch: 9, Train loss: 3.005, Val loss: 2.976,            Epoch time=1235.342s\n",
            "Loss: 2.904420564174652;\n",
            "Loss: 2.9099478726387025;\n",
            "Loss: 2.917066947778066;\n",
            "Loss: 2.920175008893013;\n",
            "Loss: 2.9215657686233523;\n",
            "Loss: 2.9233461352189383;\n",
            "Loss: 2.9255032281194415;\n",
            "Loss: 2.9273812713623046;\n",
            "Loss: 2.9299607055981953;\n",
            "Improved from 2.9759431314468383 to 2.917381847381592, saving model..\n",
            "Epoch: 10, Train loss: 2.930, Val loss: 2.917,            Epoch time=1235.178s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "    output_ids = [tokenizer_uk.token_to_id('[CLS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_uk_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_uk_mask,\n",
        "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_uk.token_to_id('[SEP]'), tokenizer_uk.token_to_id('[PAD]')]:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_uk_mask, \n",
        "        texts_en_padding_mask, texts_uk_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_uk_mask,\n",
        "                       texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    return (' '.join([tokenizer_uk.id_to_token(i).replace('##', '') for i in output_ids[1:]]))\n",
        "\n"
      ],
      "metadata": {
        "id": "nCeBvAnNAb_g"
      },
      "id": "nCeBvAnNAb_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Does this function work?')"
      ],
      "metadata": {
        "id": "hyOS9l43FVUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "555c29cd-da0c-4dec-9544-a47c4ad053d3"
      },
      "id": "hyOS9l43FVUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ця функція працює ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-MbtZmrdAb4C"
      },
      "id": "-MbtZmrdAb4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Score"
      ],
      "metadata": {
        "id": "kmJRf8MEF9fQ"
      },
      "id": "kmJRf8MEF9fQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "jXoxLZ3VGA6T"
      },
      "id": "jXoxLZ3VGA6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-uk-test.en').read().lower().splitlines()\n",
        "uk_sents_test = open('opus.en-uk-test.uk').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "nmqJUvgGGA1x"
      },
      "id": "nmqJUvgGGA1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "\n",
        "for i in range(len(en_sents_test)):\n",
        "  translations.append(translate(en_sents_test[i]))"
      ],
      "metadata": {
        "id": "IyfJJXpWGAsr"
      },
      "id": "IyfJJXpWGAsr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "  reference = tokenizer_uk.encode(t).tokens\n",
        "  hypothesis = tokenizer_uk.encode(uk_sents_test[i]).tokens\n",
        "\n",
        "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
      ],
      "metadata": {
        "id": "9jJkfvr3GAfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617b779c-564e-4c6e-94b7-4fa359b6337b"
      },
      "id": "9jJkfvr3GAfv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleus"
      ],
      "metadata": {
        "id": "tNsInhBZb_iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342108c1-e12b-4623-f2a0-51db284f1879"
      },
      "id": "tNsInhBZb_iH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34772504705825924]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xLdRLDW9HZkv"
      },
      "id": "xLdRLDW9HZkv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e_7yEWNKb_cE"
      },
      "id": "e_7yEWNKb_cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backtranslation** применяется при недостатке данных параллельных корпусов, обычно для низкоресурсных языков. Эта техника помогает, когда есть много данных на одном языке и мало на другом. Backtranslation заключается в следующем: на существующих данных обучается система машинного перевода, после чего эта система используется, чтобы перевести данные с языка перевода на исходный язык (поэтому **back**). Дальше переведенные тексты просто добавляются к малоресурному корпусу, чтобы дальше обучать более крупные системы. Таким образом, можно получить качественный машинный перевод даже на малом количестве данных исходного языка.\n",
        "\n",
        "\n",
        "На данных из семинара пример можно было бы примениить, взяв много текстов на русском и мало на английском. С помощью этих данных обучить трансформер, использовать его для перевода тестовой выборки и добавить эти переведенные тексты в новую тренировочную выборку, увеличив тем самым объем тренировочных данных. Также нужно выровнять новые полученные параллельные корпуса и обучить новый трансформер, тестовую выборку которого можно снова добавлять в новую тренировочную и так далее."
      ],
      "metadata": {
        "id": "4kMSYdrIUSZ8"
      },
      "id": "4kMSYdrIUSZ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5bf2f6",
      "metadata": {
        "id": "5a5bf2f6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "MT_homework11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}