{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperCoolCucumber/CompLing/blob/main/GPT_homework10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ 10. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f21d5e",
      "metadata": {
        "id": "76f21d5e"
      },
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 1 (8 –±–∞–ª–ª–æ–≤).\n",
        "\n",
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–æ–±—É—á–∞—Ç—å GPT –Ω–∞ –∫–∞–∫–æ–º-—Ç–æ –¥—Ä—É–≥–æ–º —Ç–µ–∫—Å—Ç–µ (–º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ª—é–±—ã–µ —Å—Ç–∏—Ö–∏ –∏–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤–µ—â–∏ –≤—Ä–æ–¥–µ –∞–Ω–µ–∫–¥–æ—Ç–æ–≤, —Ç–µ–æ—Ä–∏–π –∑–∞–≥–æ–≤–æ—Ä–æ–≤, –ø–æ—Å—Ç–æ–≤ –≤ –ø–æ–º–æ–µ—á–Ω—ã—Ö —Ç–µ–ª–µ–≥—Ä–∞–º –∫–∞–Ω–∞–ª–∞—Ö, —Ç–µ–∫—Å—Ç–æ–≤ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–æ–≤ –∏ –°–ú–ò —Å –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º). \n",
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (beam search, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, top_k –∏ —Ç–ø). –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ —Ç–µ—Ç—Ä–∞–¥–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f07bd89b",
      "metadata": {
        "id": "f07bd89b"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
      ],
      "metadata": {
        "id": "zOPGvcpDVRUC"
      },
      "id": "zOPGvcpDVRUC",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–î–∞—Ç–∞—Å–µ—Ç —Å–æ —Å—Ç–∏—Ö–∞–º–∏ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ**"
      ],
      "metadata": {
        "id": "abjRCu3aslyu"
      },
      "id": "abjRCu3aslyu"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "train_path = 'mayakovskyi.txt'\n",
        "\n",
        "train_dataset = TextDataset( tokenizer=tokenizer,file_path=train_path,block_size=64, \n",
        "                            overwrite_cache=True)\n",
        "  \n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXJ8T1FGXjdB",
        "outputId": "a34922ff-f6a5-40cd-bd1d-dbdeb70ef1a1"
      },
      "id": "QXJ8T1FGXjdB",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments( \n",
        "    output_dir= \"./finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=32,  \n",
        "    gradient_accumulation_steps=16, \n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "xwTP1F1MXjaE"
      },
      "id": "xwTP1F1MXjaE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "zS4hCCwXXjWI",
        "outputId": "793b6a86-085d-4e09-9d83-6c1059c8d984"
      },
      "id": "zS4hCCwXXjWI",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 3\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:11, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.06406107902526856, metrics={'train_runtime': 11.4528, 'train_samples_per_second': 26.195, 'train_steps_per_second': 8.732, 'total_flos': 9798451200000.0, 'train_loss': 0.06406107902526856, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ArgMax"
      ],
      "metadata": {
        "id": "pLphnxs_ssUW"
      },
      "id": "pLphnxs_ssUW"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"–Ø —Å–∫–∞–∑–∞–ª —á—Ç–æ\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(input_ids, \n",
        "                        do_sample=True,\n",
        "                        temperature=1.4,\n",
        "                        top_k=50,\n",
        "                        max_length=200,\n",
        "                        )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_lgQFKqYB08",
        "outputId": "00981698-93df-4b6a-9995-ccee37b4177d"
      },
      "id": "6_lgQFKqYB08",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–Ø —Å–∫–∞–∑–∞–ª —á—Ç–æ –º–Ω–µ –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π –º—É–∂—á–∏–Ω–∞.. –∏ —á—Ç–æ–±—ã —É –º–µ–Ω—è –±—ã–ª–∞ —Å–µ–º—å—è. –ü—É—Å—Ç—å –µ–≥–æ –≥–æ–ª–æ—Å –±—É–¥–µ—Ç –Ω–∞ –º–æ—ë–º —Å–µ—Ä–¥—Ü–µ\n",
            "–¢—ã –∏ —Å–∞–º–∞ –æ—Ç–≤–µ—Ç–∏–ª–∞ –Ω–∞ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å. –ï—Å–ª–∏ —Ç—ã —Å—á–∞—Å—Ç–ª–∏–≤–∞ —Å–æ –º–Ω–æ–π, —Ç–æ –∑–Ω–∞—á–∏—Ç –≤—Å–µ –µ—â—ë –ª—é–±–∏—à—å. –ï—Å–ª–∏ –∂–µ —Ç—ã –ø–ª–∞—á–µ—à—å, –≥–æ—Ä—é–µ—à—å, –∂–∞–ª–µ–µ—à—å, –Ω–µ –≤–µ—Ä–∏—à—å –≤ –ë–æ–≥–∞, –∑–Ω–∞–π: –Ω–∏–∫—Ç–æ –¥—Ä—É–≥–æ–π –Ω–µ –ª—é–±–∏–ª —Ç–µ–±—è.\n",
            "–ü–æ—Å–∞–¥—å –≤ –∫–ª–µ—Ç–∫—É –≤—Å–µ—Ö –≥–æ–ª—É–±–µ–π –∏ –±–µ–≥–∏ –≤–¥–∞–ª—å,\n",
            "—É–≤–∏–¥—å —Ä–∞–¥—É–≥—É –∏ —Å–æ–ª–Ω—ã—à–µ–∫.\n",
            "–ü—Ä–∏–¥–∏ –Ω–∞ –º–æ—é –º–æ–≥–∏–ª—É,\n",
            "–Ω–∞–∫–æ—Ä–º–∏ –º–µ–Ω—è –≤–ø—Ä–æ–∫.\n",
            "–ù–∞ –ª—É–≥–∞—Ö —è—Ä–º–∞—Ä–∫–æ–∫\n",
            "–ø–µ–≤—Ü—ã –¥—É–¥–∫–∏ –∑–≤–æ–Ω.\n",
            "–†–∞–¥—É–≥–∞, –¥–∞–π –±–µ–π —á–µ–ª–Ω —Å–º–µ–ª—ã–º.\n",
            "–ì—Ä—É–¥—å –º—É–∂—á–∏–Ω–∞–º, –≥–ª–æ—Ç–Ω–∏—Ç–µ\n",
            "–≤–æ–∑–¥—É—Ö–∞, –Ω–∞–¥—ã—à–∞–≤—à–∏—Å—å –∏–º.\n",
            "–ï—Å—Ç—å –º–µ–¥, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º\n",
            "–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≥–∞—Å–Ω—É—Ç –ª—å—é—Ç.\n",
            "–î–µ–≤–µ –ú–∞—Ä–∏–∏\n",
            "–∏—Å–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–µ–Ω–∏—é.\n",
            "–°–µ—Ä–¥—Ü–µ –¥–µ–≤—ã\n",
            "–ø–ª—ã–≤—ë—Ç –≤ –ª–∞–¥—É.\n",
            "–°–µ—Ä–¥—Ü–µ –¥–µ–≤—ã\n",
            "—Å—Ç—Ä–∞—à–Ω–æ, –±—ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ù–∞—á–∞–ª–æ—Å—å —Å –¥—Ä–∞–º—ã –ª–∏—á–Ω–æ–π, –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π. –ù–∞–ª–∏—Ü–æ –≥–ª–∞–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ -- –∑–≤—É—á–∞—Ç –≤—Ä–æ–¥–µ –±—ã –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ, –Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏—è—Ö –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å —Å–ª–∞–±–∞—è."
      ],
      "metadata": {
        "id": "PRuLBm2Dq6cn"
      },
      "id": "PRuLBm2Dq6cn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ],
      "metadata": {
        "id": "nzEMokHfswQa"
      },
      "id": "nzEMokHfswQa"
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=False, num_beams=20, max_length=60)\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_B4xJKvpFRd",
        "outputId": "b53e9210-4643-4d2b-a643-96ab7f515572"
      },
      "id": "s_B4xJKvpFRd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–Ø —Å–∫–∞–∑–∞–ª —á—Ç–æ-–Ω–∏–±—É–¥—å?\n",
            "–í—ã—à–µ, –≥–æ—Ä–¥—ã—Ö –≥–æ–ª–æ–≤ –≥—Ä—è–¥–∞!\n",
            "–ú—ã —Ä–∞–∑–ª–∏–≤–æ–º –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ç–æ–ø–∞\n",
            "–ø–µ—Ä–µ–º–æ–µ–º –º–∏—Ä–æ–≤ –≥–æ—Ä–æ–¥–∞.\n",
            "\n",
            "–î–Ω–µ–π –±—ã–∫ –ø–µ–≥.\n",
            "–ú–µ–¥–ª–µ–Ω–Ω–∞ –ª–µ—Ç –∞—Ä–±–∞.\n",
            "–ù–∞—à –±–æ–≥ –±–µ–≥.\n",
            "–°–µ—Ä–¥—Ü–µ –Ω–∞—à –±–∞—Ä–∞–±–∞–Ω.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü–ª–æ—Ö–æ –∑–Ω–∞—é –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ, –Ω–æ –≤—Ä–æ–¥–µ –¥–æ–≤–æ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–µ."
      ],
      "metadata": {
        "id": "EoQ4QkjEsHI3"
      },
      "id": "EoQ4QkjEsHI3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature"
      ],
      "metadata": {
        "id": "bWfYcZ8ds0Vt"
      },
      "id": "bWfYcZ8ds0Vt"
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(input_ids, do_sample=True, temperature=0.1, max_length=50)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZJr-mLaYBw3",
        "outputId": "fbce7b74-2519-4c72-e622-3ac87a652bbd"
      },
      "id": "tZJr-mLaYBw3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "–Ø —Å–∫–∞–∑–∞–ª —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ: ¬´–Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç¬ª.\n",
            "\n",
            "‚Äì¬†–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç?\n",
            "\n",
            "‚Äì¬†–ú—ã –Ω–µ –º–æ–∂–µ–º –ø–æ–Ω—è—Ç—å, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç.\n",
            "\n",
            "‚Äì¬†–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç?\n",
            "\n",
            "‚Äì¬†–ú—ã –Ω–µ –º–æ–∂–µ–º –ø–æ–Ω—è—Ç—å,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "relatable"
      ],
      "metadata": {
        "id": "VD5h_Z7Kq1j6"
      },
      "id": "VD5h_Z7Kq1j6"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aaycJvORYBQ6"
      },
      "id": "aaycJvORYBQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ae8437e8",
      "metadata": {
        "id": "ae8437e8"
      },
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ  2 (2 –±–∞–ª–ª–∞)\n",
        "\n",
        "–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n",
        "\n",
        "1) –í –∫–∞–∫–∏—Ö —Å—Ç–∞—Ç—å—è –±—ã–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã GPT-1, GPT-2, GPT-3?\n",
        "\n",
        "2) –ö–∞–∫ —Å–æ–±–∏—Ä–∞–ª—Å—è –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å –¥–ª—è GPT-3? –ö–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Å–æ–∑–¥–∞—Ç–µ–ª–∏ —Å—Ç–∞—Ä–∞–ª–∏—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) –í—Å–µ —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –≥—É–≥–ª–æ–≤—Å–∫–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–µ–π OpenAI.\n",
        "\n",
        "GPT-1: [Improving Language Understanding\n",
        "by Generative Pre-Training](https://www.gwern.net/docs/www/s3-us-west-2.amazonaws.com/d73fdc5ffa8627bce44dcda2fc012da638ffb158.pdf) -- 2018 –≥–æ–¥\n",
        "\n",
        "GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) -- 2019 –≥–æ–¥\n",
        "\n",
        "GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) -- 2020 –≥–æ–¥\n",
        "\n",
        "2) –°–æ–∑–¥–∞—Ç–µ–ª–∏ –≤–∑—è–ª–∏ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö -- Common Crawl dataset. –ß—Ç–æ–±—ã –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è–ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ:\n",
        "- –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ—Ä–ø—É—Å–∞–º–∏ –≤—ã—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "- –ø—Ä–∏–º–µ–Ω–∏–ª–∏ fuzzy deduplication: —É–¥–∞–ª—è–ª–∏ –¥—É–±–ª–∏ –ø–æ—á—Ç–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤\n",
        "- –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ –∫–æ—Ä–ø—É—Å –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ç–µ–∫—Å—Ç–∞–º–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤."
      ],
      "metadata": {
        "id": "tMLbyvDTvXsa"
      },
      "id": "tMLbyvDTvXsa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "GPT_homework10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}