{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ca41e2",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0588a6",
   "metadata": {},
   "source": [
    "### Задание 1 Реализовать алгоритм Леска и проверить его на реальном датасете (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5283f",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f082cd",
   "metadata": {},
   "source": [
    "Реализуйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a837f73-7a70-42bd-bcb4-43a33bf943b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3424a7f8-8d66-4fdf-9f75-962c620377b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    \n",
    "    sentence = [word for word in sentence.split() if word not in punct]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca7909ef-1915-4d5c-8d28-d22927ac39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(synset, sentence):\n",
    "    gloss = set(WordPunctTokenizer().tokenize(synset.definition()))\n",
    "    for i in synset.examples():\n",
    "        gloss.union(i)\n",
    "    # gloss = gloss.difference( functionwords )\n",
    "    # sentence = sentence.difference( functionwords )\n",
    "    return len(gloss.intersection(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668e6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    word = wn.morphy(word)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        overlap = compute_overlap(synset, sentence)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "        \n",
    "        \n",
    "    return bestsense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4309c",
   "metadata": {},
   "source": [
    "**Проверьте насколько хорошо работает такой метод на датасете из семинара**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa7ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf01fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b8806",
   "metadata": {},
   "source": [
    "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
    "\n",
    "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "bfa5f76b-11db-41ec-a606-4113c0f718aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'how', 'How'],\n",
       " ['long%3:00:02::', 'long', 'long'],\n",
       " ['', 'have', 'has'],\n",
       " ['', 'it', 'it'],\n",
       " ['be%2:42:03::', 'be', 'been'],\n",
       " ['', 'since', 'since'],\n",
       " ['', 'you', 'you'],\n",
       " ['review%2:31:00::', 'review', 'reviewed'],\n",
       " ['', 'the', 'the'],\n",
       " ['objective%1:09:00::', 'objective', 'objectives'],\n",
       " ['', 'of', 'of'],\n",
       " ['', 'you', 'your'],\n",
       " ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
       " ['', 'and', 'and'],\n",
       " ['service%1:04:07::', 'service', 'service'],\n",
       " ['program%1:09:01::', 'program', 'program'],\n",
       " ['', '?', '?']]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "018f68e1-b219-460d-96f7-b4b33429f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long',\n",
       " 'be',\n",
       " 'review',\n",
       " 'objective',\n",
       " 'benefit',\n",
       " 'service',\n",
       " 'program',\n",
       " 'permit',\n",
       " 'become',\n",
       " 'giveaway']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переменная для всех слов без wordnet-разметки (для функции lesk)\n",
    "words = list(map(lambda a: list(map(lambda a: a.split('%')[0],\n",
    "                filter(lambda s: '%' in s, chain(*a)))), corpus_wsd))\n",
    "words = list(itertools.chain.from_iterable(words))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "fc88e93e-b90c-4203-8714-587e897928b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = lambda item: list(map(list, zip(*item)))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "7cf7b61e-7341-4394-9b35-6fd660f899d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переменная для всех предложений (для функции lesk)\n",
    "sents = list(map(\n",
    "    lambda s: ' '.join(s), \n",
    "    list(map(transposed, corpus_wsd)))\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "6dd7858c-98d6-4f6f-94c7-c872e55b198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How long has it been since you reviewed the objectives of your benefit and service program ?',\n",
       " 'Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and , consequently , increased productivity ?',\n",
       " 'What effort do you make to assess results of your program ?',\n",
       " 'Do you measure its relation to reduced absenteeism , turnover , accidents , and grievances , and to improved quality and output ?',\n",
       " 'Have you set specific objectives for your employee publication ?']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7f6067a8-1235-44ed-b6ae-eedcff66a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем предсказания от леска\n",
    "lesk_senses = []\n",
    "\n",
    "for w, s in zip(words, sents):\n",
    "    for item in w:\n",
    "        try:\n",
    "            lesk_senses.append(lesk(item, s))\n",
    "        except AttributeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9aa3e84c-f957-4abc-a782-cc15432219a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk_senses[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a207674e-17b4-4a0d-8f10-377f57d1de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# достаем все слова с wordnet-разметкой, чтобы проверить точность леска на них\n",
    "\n",
    "wordnet_tagged_words = list(map(lambda a: list(filter(lambda s: '%' in s, chain(*a))), corpus_wsd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6aa67db0-512a-4cc7-aea2-6dc7b8f4c5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long%3:00:02::',\n",
       " 'be%2:42:03::',\n",
       " 'review%2:31:00::',\n",
       " 'objective%1:09:00::',\n",
       " 'benefit%1:21:00::',\n",
       " 'service%1:04:07::',\n",
       " 'program%1:09:01::']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_tagged_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5d73f7d8-8006-4ff2-92fc-e97b5cce08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем предсказания от wordnet\n",
    "wordnet_senses = []\n",
    "\n",
    "for w in wordnet_tagged_words:\n",
    "    for ww in w:\n",
    "        try:\n",
    "            wordnet_senses.append(wn.lemma_from_key(ww).synset())\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "86f3ce60-2915-460c-97f2-63f8253e4d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('long.a.01')\n",
      "[Synset('hanker.v.01'), Synset('long.a.01'), Synset('long.a.02'), Synset('long.s.03'), Synset('retentive.a.01'), Synset('long.a.05'), Synset('long.a.06'), Synset('long.s.07'), Synset('farseeing.s.02'), Synset('long.s.09'), Synset('long.r.01'), Synset('long.r.02')]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_senses[0])\n",
    "print(wn.synsets('long'))\n",
    "print(lesk_senses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "4da8224f-da0d-48d1-9efc-0e6e17448f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(len(words)):\n",
    "    try:\n",
    "        acc.append(wordnet_senses[i] == wn.synsets(words[i])[lesk_senses[i]])\n",
    "    except IndexError:\n",
    "        acc.append(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075dc8d-b3a9-42ba-ac30-45aec9282984",
   "metadata": {},
   "source": [
    "#### Точность получается такая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "de31a197-1b5d-4efb-b331-e56a7d56ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.198421716383432"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.count_nonzero(acc)/len(acc)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4aa7de-507e-4d04-aef3-e12098066a6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e08e4-c8b1-45fc-8ad7-373c1a070d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654b174-22bc-465a-acd7-7239d3a5be05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48b45d4",
   "metadata": {},
   "source": [
    "### Задание 2* (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ef129",
   "metadata": {},
   "source": [
    "В семинаре для WSI на данных Диалога использовался только Fastext. Попробуйте заменить его на адаграм (обучите свою модель или используйте предобученную out.pkl или https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib), а также поэкспериментируйте с разными алгоритмами кластеризации и их параметрами (можно использовать только те алгоритмы, которые обучаются достаточно быстро)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e5ea3",
   "metadata": {},
   "source": [
    "Для каждого эксперимента рассчитайте ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29181b2f-4286-4030-98fb-43d89fc38076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b368ba5-3a3c-4668-94e4-813cb6a5f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "adagram = adagram.VectorModel.load('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8b6a93-e9f7-4095-89a7-fd97465d15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40dd7fab-e352-4f6c-a470-c71e61239a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a258b2df-1ed1-4d65-ba10-4e528032e826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-5, 339-344</td>\n",
       "      <td>замок владимира мономаха в любече . многочисле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-16, 17-22, 188-193</td>\n",
       "      <td>шильонский замок замок шильйон ( ) , известный...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>299-304</td>\n",
       "      <td>проведения архитектурно - археологических рабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111-116</td>\n",
       "      <td>топи с . , л . белокуров легенда о завещании м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134-139, 262-267</td>\n",
       "      <td>великий князь литовский гедимин после успешной...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   context_id   word  gold_sense_id  predict_sense_id              positions  \\\n",
       "0           1  замок              1               NaN           0-5, 339-344   \n",
       "1           2  замок              1               NaN  11-16, 17-22, 188-193   \n",
       "2           3  замок              1               NaN                299-304   \n",
       "3           4  замок              1               NaN                111-116   \n",
       "4           5  замок              1               NaN       134-139, 262-267   \n",
       "\n",
       "                                             context  \n",
       "0  замок владимира мономаха в любече . многочисле...  \n",
       "1  шильонский замок замок шильйон ( ) , известный...  \n",
       "2  проведения архитектурно - археологических рабо...  \n",
       "3  топи с . , л . белокуров легенда о завещании м...  \n",
       "4  великий князь литовский гедимин после успешной...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa75c50-e452-4ddf-b15a-60febfbcd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = data.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a942822b-d43d-41b5-9125-4d8a4d23aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858c0c15-222a-4aa0-aa5a-92f0a8cd9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    \n",
    "    vectors = []    \n",
    "    for i,word in enumerate(text):\n",
    "        try:\n",
    "            v = model.sense_vector(word, 0)\n",
    "            vectors.append(v)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    vectors = np.vstack(vectors)#.reshape(-1, 3)\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64191031-3126-4b8e-8127-ba8066d7a829",
   "metadata": {},
   "source": [
    "### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c9f785-b25f-4259-a325-8a5716f9f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1763034369313108\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].apply(normalize)\n",
    "    X = np.zeros((len(texts), 100))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = [word for word in text if word != key]\n",
    "        X[i] = get_embedding(text, adagram, 100)\n",
    "        \n",
    "    cluster = DBSCAN(min_samples=5, eps=0.9)\n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1\n",
    "    \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb364469-0ccf-4f10-ae54-ae671d065982",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "227fb305-b4f4-450d-8272-cc191eaed323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04891654657641001\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].apply(normalize)\n",
    "    X = np.zeros((len(texts), 100))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = [word for word in text if word != key]\n",
    "        X[i] = get_embedding(text, adagram, 100)\n",
    "        \n",
    "    cluster = KMeans(n_clusters=10)\n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1\n",
    "    \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b055e3e-711a-48fb-a6c1-30f4a3678c17",
   "metadata": {},
   "source": [
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c14a26d-f0a4-4109-89e1-07aa13c27348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017448317558380694\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].apply(normalize)\n",
    "    X = np.zeros((len(texts), 100))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = [word for word in text if word != key]\n",
    "        X[i] = get_embedding(text, adagram, 100)\n",
    "        \n",
    "    cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1\n",
    "    \n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
